From 35fd9839d89117a50dc0a99f54f8bb673caa9f5b Mon Sep 17 00:00:00 2001
From: Mehran Sarmadi <mehran.sarmadi16@gmail.com>
Date: Mon, 8 May 2023 21:45:01 +0000
Subject: [PATCH] MY changes

---
 .../models/roberta/modeling_roberta.py        | 22 +++++++++++++++++++
 1 file changed, 22 insertions(+)

diff --git a/src/transformers/models/roberta/modeling_roberta.py b/src/transformers/models/roberta/modeling_roberta.py
index 75b728af6..a0045edf6 100644
--- a/src/transformers/models/roberta/modeling_roberta.py
+++ b/src/transformers/models/roberta/modeling_roberta.py
@@ -289,11 +289,22 @@ class RobertaSelfOutput(nn.Module):
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
+        # start adapter related part
+        self.adapter_down = nn.Linear(config.hidden_size, 128)
+        self.adapter_relu = nn.ReLU()
+        self.adapter_up = nn.Linear(128, config.hidden_size)
+        # finish adapter related part
         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.dropout = nn.Dropout(config.hidden_dropout_prob)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
+        # start adapter related part
+        hidden_states_adapter = self.adapter_down(hidden_states)
+        hidden_states_adapter = self.adapter_relu(hidden_states_adapter)
+        hidden_states_adapter = self.adapter_up(hidden_states_adapter)
+        hidden_states = hidden_states + hidden_states_adapter  # because of skip-connection
+        # finish adapter related part
         hidden_states = self.dropout(hidden_states)
         hidden_states = self.LayerNorm(hidden_states + input_tensor)
         return hidden_states
@@ -370,11 +381,22 @@ class RobertaOutput(nn.Module):
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
+        # start adapter related part
+        self.adapter_down = nn.Linear(config.hidden_size, 128)
+        self.adapter_relu = nn.ReLU()
+        self.adapter_up = nn.Linear(128, config.hidden_size)
+        # finish adapter related part
         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
         self.dropout = nn.Dropout(config.hidden_dropout_prob)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
+        # start adapter related part
+        hidden_states_adapter = self.adapter_down(hidden_states)
+        hidden_states_adapter = self.adapter_relu(hidden_states_adapter)
+        hidden_states_adapter = self.adapter_up(hidden_states_adapter)
+        hidden_states = hidden_states + hidden_states_adapter  # because of skip-connection
+        # finish adapter related part
         hidden_states = self.dropout(hidden_states)
         hidden_states = self.LayerNorm(hidden_states + input_tensor)
         return hidden_states
-- 
2.25.1

